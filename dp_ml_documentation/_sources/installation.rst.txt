.. dp_ml documentation master file, created by
   sphinx-quickstart on Tue Dec 12 09:58:51 2017.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

installation guide
=================================

.. image:: /figures/dp_ml_logo.png
   :align: center

personal computer
-------------------

We recommend that you use Python 3.5 and above. 
dp_ml depends on several packages. 
Please install these first:

   - `tensorflow <https://www.tensorflow.org/install/>`_
   - `scikit-learn <http://scikit-learn.org/stable/install.html>`_
   - `h5py <http://docs.h5py.org/en/latest/build.html>`_

We use several other packages too. All of these can be installed by running the following commands:

.. code-block:: sh
	
	pip install tensorflow-gpu
	pip install scikit-learn
	pip install h5py
	pip install seaborn
	pip install scipy
	pip install h5py
	pip install matplotlib

Note that if your system does not have a GPU, tensorflow is installed with :code:`pip install tensorflow`.

When ready, navigate in a terminal to a folder where you would like to store the program.
(e.g. :code:`cd <user>/Documents/PropertyEstimation`)
Then run the following commands to install dp_ml:

.. code-block:: sh
   
   git clone https://github.com/dcgarrett/dielectric_estimation_ml.git
   cd dielectric_estimation_ml
   pip install .

Next, download the data we have generated.
You can also generate your own data using your simulation software if you would prefer (see the Tutorial section).
The filesize is quite large (~700 MB).
The data can be downloaded from `here <https://www.davidgarrett.ca/dielectric_estimation_ml/data/dp_ml_data.hdf5>`_.
Store it in a convenient location, remembering the file path.

Now we can assign some user parameters.
Find the path where the package was installed by running:

.. code-block:: sh 

	python
	>>> import site; site.getsitepackages()

Navigate to this folder, then in a text editor, open the file :code:`config.py`, which is located in the dp_ml folder.
Here are user file paths for Windows, Mac and Linux.
Change the file paths corresponding to your system only.
This was done so you can work on multiple systems without having to change the file path each time.

- :code:`HDF5Path` corresponds to the path to the file you just downloaded (or to the folder you will fill with your own data).
- :code:`HDF5Filename` corresponds to the path to the name of this file.
- :code:`dataPath` and :code:`calPath` correspond to the folders where you store the raw data generated by your simulation tool. These are only necessary to assign if you plan on generating your own data. For instance, we use Sim4Life which will output S-parameters in .xls files. 

Finally, perform a test of the installation.
Navigate to the folder of the installation (e.g. :code:`cd <user>/Documents/PropertyEstimation/dielectric_estimation_ml`), then run:

.. code-block:: sh

	python -m unittest dp_ml/test_dpml.py

Make sure you get the 'OK' result from the unit tests.
Otherwise, depending on the error, you may need to check your filepaths etc.


computing cluster
------------------

For improved performance, it is also possible to install this package on computing clusters such as those of Compute Canada or WestGrid.
This can be used both for developing the machine learning models, and for simulating the training data.
Once you have an account on your desired platform, follow the steps below.

for machine learning models:
``````````````````````````````

First get a working virtual environment for Python.
Check which versions of Python are available on your machine:

.. code-block:: sh

        module avail python

It is recommended to install versions of Python 3.5 and up.
Load the module using:

.. code-block:: sh

        module load python/3.5.4

In your home directory (or any other desired directory) run:

.. code-block:: sh

	virtualenv ENV
	source ENV/bin/activate

where :code:`ENV` is the desired name for your virtual environment (I called mine :code:`tensorflow`).

Now install tensorflow for use with GPU, and the other required packages:

.. code-block:: sh

	pip install tensorflow-gpu
	pip install scikit-learn
	pip install seaborn
	pip install scipy
	pip install h5py
	pip install matplotlib

Next, install the dp_ml package:

.. code-block:: sh

    git clone https://github.com/dcgarrett/dielectric_estimation_ml.git
    cd dielectric_estimation_ml
    pip install .

Then use a secure file transfer protocol (SFTP) program like PuTTY or CyberDuck to transfer the data file to the server.
Follow the same steps as above to configure and test the installation.


for generating training simulations:
``````````````````````````````````````

These computing clusters can work very well for generating the training data also.
Our group uses Sim4Life -- these instructions will therefore focus on its installation.
We are also using Compute Canada's Cedar.
This uses CentOS 7, and has many high-performance GPUs.
If your computing cluster is a different operating system, these instructions may require modification.

First transfer the program file into your directory on the computing cluster.
This can again be done using a SFTP program.
On the computing cluster, navigate to this folder and extract the file using:

.. code-block:: sh
	
	tar xvzf file.tar.gz

where :code:`file` is the name of the program file.
I found I needed to download this `library <https://pkgs.org/download/libicuuc.so.50()(64bit)>`_. 
Download the version appropriate for the computing cluster, then extract all the files on your local computer.
Copy all of them with SFTP to the :code:`bin` folder of the program on the cluster.
Next navigate to the folder of the program file (one level up from :code:`bin`.
Modify all these files to be executable, and update your paths by running:

.. code-block:: sh
	
	chmod -R +x bin/
	setrpaths.sh --path <path_to_bin_folder>

To run Sim4Life simulations, first you need to generate the input HDF5 files.
As of now, these unfortunately need to be performed using the GUI on a Windows computer. 
You could set up Python scripts to automate this, then send over the input files in a batch to the cluster.

Next, Cedar uses SLURM scripts to send jobs to the allocated resources.
Some clusters may use other standards, such as qsub.
Here you define which code to run, and any other parameters for the job.
An example SLURM script is shown below:

.. code-block:: sh

	#SBATCH --gres=gpu:1        # request GPU "generic resource"
	#SBATCH --cpus-per-task=6   # maximum CPU cores per GPU request: 6 on Cedar, 16 on Graham.
	#SBATCH --mem=32000M        # memory per node
	#SBATCH --time=0-03:00      # time (DD-HH:MM)
	#SBATCH --output=%N-%j.out  # %N for node name, %j for jobID

	LM_LICENSE_FILE=@<server_ip>
	AX_USE_UNSUPPORTED_CARDS=1
	SEMCADX_CUDA_ADDITIONAL_CARDS="Tesla P100-PCIE-12GB"
	LD_LIBRARY_PATH="<path_to_bin_folder>"

	export LM_LICENSE_FILE
	export AX_USE_UNSUPPORTED_CARDS
	export SEMCADX_CUDA_ADDITIONAL_CARDS
	export LD_LIBRARY_PATH

	nvidia-smi

	./iSolve <input_h5_file>

Replace :code:`server_ip` with the IP address of your group's license server.
:code:`path_to_bin_folder` is the same path used above, to the bin folder of the program file.
Finally, :code:`input_h5_file` is the input file generated from the desktop version of your software.
More details will be shown on doing this in the :ref:`tutorial` section.











